TrainingArguments:
 bits: 4
 disable_exllama: True
 model_id: "TheBloke/zephyr-7B-alpha-GPTQ"
 device_map: "auto"
 use_cache: False
 output_dir: "zephyr-support-chatbot"
 batch_size: 8
 grad_accumulation_steps: 1
 optimizer: "paged_adamw_32bit"
 lr: 0.0002
 lr_scheduler: "cosine"
 save_strategy: "epoch"
 logging_steps: 50
 num_train_epoch: 1
 max_steps: 250
 fp16: True
 push_to_hub: True
 max_seq_length: 512
 packing: False



LoraConfiguration:
 lora_r: 16
 lora_alpha: 16 
 lora_dropout: 0.05
 bias: "none" 
 task_type: "CAUSAL_LM"
 target_modules: ["q_proj", "v_proj"]