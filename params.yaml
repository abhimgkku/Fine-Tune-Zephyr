TrainingArguments:
 num_train_epochs: 1
 per_device_train_batch_size: 4
 gradient_accumulation_steps: 1
 per_device_eval_batch_size: 4
 gradient_checkpointing: True
 optim: "paged_adamw_32bit"
 save_steps: 0
 logging_steps: 50
 learning_rate: 2e-4
 weight_decay: 0.001
 fp16: True
 bf16: False
 max_grad_norm: 0.3
 max_steps: 250
 warmup_ratio: 0.03
 group_by_length: True
 lr_scheduler_type: "cosine"
 report_to: "tensorboard"
 max_seq_length: None
 packing: False
 output_dir : "zephyr-support-chatbot"
 save_strategy : "epoch"
 push_to_hub : True


LoraConfiguration:
 load_in_4bit: True
 bnb_4bit_quant_type: "nf4"
 bnb_4bit_compute_dtype: "float16"
 bnb_4bit_use_double_quant: False
 lora_alpha: 16
 lora_dropout: 0.5
 lora_r: 16
 bias: "none"
 task_type: "CAUSAL_LM"
 target_modules: ["q_proj", "v_proj"]
#  use_nested_quant: False
#  use_4bit: True